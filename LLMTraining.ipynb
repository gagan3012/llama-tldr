{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8Y6QqbbEA76z8ztICILWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gagan3012/llama-tdlr/blob/master/LLMTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructional Finetuning"
      ],
      "metadata": {
        "id": "2IMiAJVBVRA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
      ],
      "metadata": {
        "id": "He1GzBCXUuSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuyPeBpgUrMQ"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from random import randrange\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, AutoTokenizer\n",
        "\n",
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alpaca Format"
      ],
      "metadata": {
        "id": "5dndJUDcV9W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruction(sample):\n",
        "    if sample['context'] == '':\n",
        "        sample['text'] = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "        ### Instruction:\n",
        "        {sample['instruction']}\n",
        "\n",
        "        ### Response:\n",
        "        {sample['response']}\n",
        "        \"\"\"\n",
        "    else:\n",
        "        sample['text'] = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "        ### Instruction:\n",
        "        {sample['instruction']}\n",
        "\n",
        "        ### Input:\n",
        "        {sample['context']}\n",
        "\n",
        "        ### Response:\n",
        "        {sample['response']}\n",
        "        \"\"\"\n",
        "    return sample"
      ],
      "metadata": {
        "id": "E9iAVqG9UyAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(format_instruction)\n",
        "print(f\"dataset size: {len(dataset)}\")\n",
        "print(dataset[randrange(len(dataset))])"
      ],
      "metadata": {
        "id": "hvRhi5XSU0Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Loading"
      ],
      "metadata": {
        "id": "CddJusPqWG0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "knztFXSRU2oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "n1radsnNWJ6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"llama-2-dolly\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    save_total_limit=2,\n",
        "    save_steps=1000,\n",
        ")"
      ],
      "metadata": {
        "id": "E7_dq-IWU7w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "Hpvjv9r3WOFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048  # max sequence length for model and packing of the dataset\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True,\n",
        "    args=args,\n",
        "    dataset_text_field=\"text\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "J9GmuxMoVI4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "ucQRLqiCVNxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJKNBHBTVNsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lora PEFT"
      ],
      "metadata": {
        "id": "Cgot2qLmV3Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
      ],
      "metadata": {
        "id": "AKCrxyTWWWDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK6-0mEjWWDq"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from random import randrange\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, AutoTokenizer\n",
        "\n",
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alpaca Format"
      ],
      "metadata": {
        "id": "vG3dUziqWWDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruction(sample):\n",
        "    if sample['context'] == '':\n",
        "        sample['text'] = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "        ### Instruction:\n",
        "        {sample['instruction']}\n",
        "\n",
        "        ### Response:\n",
        "        {sample['response']}\n",
        "        \"\"\"\n",
        "    else:\n",
        "        sample['text'] = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "        ### Instruction:\n",
        "        {sample['instruction']}\n",
        "\n",
        "        ### Input:\n",
        "        {sample['context']}\n",
        "\n",
        "        ### Response:\n",
        "        {sample['response']}\n",
        "        \"\"\"\n",
        "    return sample"
      ],
      "metadata": {
        "id": "XSs4-pbfWWDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(format_instruction)\n",
        "print(f\"dataset size: {len(dataset)}\")\n",
        "print(dataset[randrange(len(dataset))])"
      ],
      "metadata": {
        "id": "Ng9U3dxfWWDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Loading"
      ],
      "metadata": {
        "id": "kmHX60kVWWDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "DcfAik0VWWDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "S0URtbyuWcmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "xUOBa36zWWDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"llama-2-dolly\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    save_total_limit=2,\n",
        "    save_steps=1000,\n",
        ")"
      ],
      "metadata": {
        "id": "aSDpCEO9WWDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "Dq5D17qaWWDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(\n",
        "            args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        kwargs[\"model\"].save_pretrained(checkpoint_folder)\n",
        "\n",
        "        pytorch_model_path = os.path.join(\n",
        "            checkpoint_folder, \"pytorch_model.bin\")\n",
        "        torch.save({}, pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "class LoadBestPeftModelCallback(TrainerCallback):\n",
        "    def on_train_end(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        print(\n",
        "            f\"Loading best peft model from {state.best_model_checkpoint} (score: {state.best_metric}).\")\n",
        "        best_model_path = os.path.join(\n",
        "            state.best_model_checkpoint, \"adapter_model.bin\")\n",
        "        adapters_weights = torch.load(best_model_path)\n",
        "        model = kwargs[\"model\"]\n",
        "        set_peft_model_state_dict(model, adapters_weights)\n",
        "        return control"
      ],
      "metadata": {
        "id": "dFPjKF_xWv56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    max_seq_length=max_seq_length,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True,\n",
        "    args=args,\n",
        "    dataset_text_field=\"text\",\n",
        "    callbacks=[LoadBestPeftModelCallback(), SavePeftModelCallback()],\n",
        ")"
      ],
      "metadata": {
        "id": "HPxwCM0UWWDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "9JQqk1ygWWDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "A1D8AjYDWWDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge and Save Model"
      ],
      "metadata": {
        "id": "GsZfQqPZXKll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    args.output_dir,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# Merge LoRA and base model\n",
        "merged_model = model.merge_and_unload()\n",
        "\n",
        "# Save the merged model\n",
        "merged_model.save_pretrained(args.output_dir, safe_serialization=True)\n",
        "tokenizer.save_pretrained(args.output_dir)"
      ],
      "metadata": {
        "id": "H1ICypBXXKXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OUlNuQwwXWX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QLora PEFT"
      ],
      "metadata": {
        "id": "u2XL8_tZXWgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
      ],
      "metadata": {
        "id": "erilpNAwXWgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaBMoJcsXWgp"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from random import randrange\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, AutoTokenizer\n",
        "\n",
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alpaca Format"
      ],
      "metadata": {
        "id": "SFhe6jM_XWgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruction(sample):\n",
        "    if sample['context'] == '':\n",
        "        sample['text'] = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "        ### Instruction:\n",
        "        {sample['instruction']}\n",
        "\n",
        "        ### Response:\n",
        "        {sample['response']}\n",
        "        \"\"\"\n",
        "    else:\n",
        "        sample['text'] = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "        ### Instruction:\n",
        "        {sample['instruction']}\n",
        "\n",
        "        ### Input:\n",
        "        {sample['context']}\n",
        "\n",
        "        ### Response:\n",
        "        {sample['response']}\n",
        "        \"\"\"\n",
        "    return sample"
      ],
      "metadata": {
        "id": "vpqjm9moXWgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(format_instruction)\n",
        "print(f\"dataset size: {len(dataset)}\")\n",
        "print(dataset[randrange(len(dataset))])"
      ],
      "metadata": {
        "id": "3Yy18fWhXWgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Loading"
      ],
      "metadata": {
        "id": "PVme4Za8XWgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             use_cache=False,\n",
        "                                             device_map=\"auto\")\n",
        "model.config.pretraining_tp = 1\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "PDBkys9TXWgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "bqhH1aldXWgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "8LBPBVoIXWgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"llama-2-dolly\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    save_total_limit=2,\n",
        "    save_steps=1000,\n",
        ")"
      ],
      "metadata": {
        "id": "Gj5pYFqdXWgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "E5AxTmylXWgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(\n",
        "            args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        kwargs[\"model\"].save_pretrained(checkpoint_folder)\n",
        "\n",
        "        pytorch_model_path = os.path.join(\n",
        "            checkpoint_folder, \"pytorch_model.bin\")\n",
        "        torch.save({}, pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "class LoadBestPeftModelCallback(TrainerCallback):\n",
        "    def on_train_end(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        print(\n",
        "            f\"Loading best peft model from {state.best_model_checkpoint} (score: {state.best_metric}).\")\n",
        "        best_model_path = os.path.join(\n",
        "            state.best_model_checkpoint, \"adapter_model.bin\")\n",
        "        adapters_weights = torch.load(best_model_path)\n",
        "        model = kwargs[\"model\"]\n",
        "        set_peft_model_state_dict(model, adapters_weights)\n",
        "        return control"
      ],
      "metadata": {
        "id": "85tHkWUrXWgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    max_seq_length=max_seq_length,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True,\n",
        "    args=args,\n",
        "    dataset_text_field=\"text\",\n",
        "    callbacks=[LoadBestPeftModelCallback(), SavePeftModelCallback()],\n",
        ")"
      ],
      "metadata": {
        "id": "Ejx2z-b4XWgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Qnw_YNBgXWgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "3_fRxJH6XWgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge and Save Model"
      ],
      "metadata": {
        "id": "TXWlxgtYXWgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    args.output_dir,\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "\n",
        "# Merge LoRA and base model\n",
        "merged_model = model.merge_and_unload()\n",
        "\n",
        "# Save the merged model\n",
        "merged_model.save_pretrained(args.output_dir, safe_serialization=True)\n",
        "tokenizer.save_pretrained(args.output_dir)"
      ],
      "metadata": {
        "id": "o4K1KhQuXWgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "yKa0RoCuXvD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm"
      ],
      "metadata": {
        "id": "e0F0r89VXwkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "# Sample prompts.\n",
        "prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The president of the United States is\",\n",
        "    \"The capital of France is\",\n",
        "    \"The future of AI is\",\n",
        "]\n",
        "# Create a sampling params object.\n",
        "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=100)\n",
        "\n",
        "# Create an LLM.\n",
        "llm = LLM(model=args.output_dir)\n",
        "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
        "# that contain the prompt, generated text, and other information.\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
      ],
      "metadata": {
        "id": "XYZ7t7ZyXynj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
